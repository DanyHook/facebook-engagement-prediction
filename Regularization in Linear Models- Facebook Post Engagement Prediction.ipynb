{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📊 Regularization in Linear Models: Facebook Post Engagement Prediction\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "In this project, we explore the importance of regularization techniques in linear models by predicting Facebook post interactions for a cosmetics brand. We compare three regression models:\n",
    "\n",
    "- **Linear Regression** (no regularization)\n",
    "- **Lasso Regression** (L1 regularization)\n",
    "- **Ridge Regression** (L2 regularization)\n",
    "\n",
    "The goal is to understand how regularization affects coefficient stability, model complexity, and overall performance—especially in the presence of multicollinearity among features.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- Preprocess and standardize the dataset using `StandardScaler`\n",
    "- Train and evaluate Linear, Lasso, and Ridge regression models\n",
    "- Analyze and visualize the impact of regularization on feature weights\n",
    "- Compare model performance using R² score and Mean Squared Error (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 📂 Load the dataset\n",
    "df = pd.read_csv('/mnt/data/dataset_facebook_cosmetics_us.csv', sep=';')\n",
    "\n",
    "# 🎯 Separate features and target\n",
    "X = df.drop('Total Interactions', axis=1)\n",
    "y = df['Total Interactions']\n",
    "\n",
    "# ✂️ Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# ⚖️ Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 🤖 Train models\n",
    "lr = LinearRegression()\n",
    "lasso = Lasso(alpha=0.1)\n",
    "ridge = Ridge(alpha=1.0)\n",
    "\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "lasso.fit(X_train_scaled, y_train)\n",
    "ridge.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 📌 Coefficients comparison\n",
    "feature_names = X.columns\n",
    "coef_lr = lr.coef_\n",
    "coef_lasso = lasso.coef_\n",
    "coef_ridge = ridge.coef_\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Linear': coef_lr,\n",
    "    'Lasso (L1)': coef_lasso,\n",
    "    'Ridge (L2)': coef_ridge\n",
    "})\n",
    "print(\"🔎 Model Coefficients Comparison\")\n",
    "print(coef_df)\n",
    "\n",
    "# 📊 Plot coefficient comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "x = np.arange(len(feature_names))\n",
    "width = 0.25\n",
    "plt.bar(x - width, coef_lr, width, label='Linear')\n",
    "plt.bar(x, coef_lasso, width, label='Lasso (L1)')\n",
    "plt.bar(x + width, coef_ridge, width, label='Ridge (L2)')\n",
    "plt.xticks(ticks=x, labels=feature_names, rotation=45, ha='right')\n",
    "plt.ylabel('Coefficient Value')\n",
    "plt.title('Comparison of Model Coefficients')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 🧪 Evaluate models\n",
    "y_pred_lr = lr.predict(X_test_scaled)\n",
    "y_pred_lasso = lasso.predict(X_test_scaled)\n",
    "y_pred_ridge = ridge.predict(X_test_scaled)\n",
    "\n",
    "metrics = pd.DataFrame({\n",
    "    'Model': ['Linear Regression', 'Lasso (L1)', 'Ridge (L2)'],\n",
    "    'R² Score': [\n",
    "        r2_score(y_test, y_pred_lr),\n",
    "        r2_score(y_test, y_pred_lasso),\n",
    "        r2_score(y_test, y_pred_ridge)\n",
    "    ],\n",
    "    'MSE': [\n",
    "        mean_squared_error(y_test, y_pred_lr),\n",
    "        mean_squared_error(y_test, y_pred_lasso),\n",
    "        mean_squared_error(y_test, y_pred_ridge)\n",
    "    ]\n",
    "})\n",
    "print(\"📈 Model Evaluation Metrics\")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ Conclusions\n",
    "\n",
    "- **Linear Regression** performs well but is susceptible to multicollinearity, leading to potentially large and unstable coefficients.\n",
    "- **Lasso Regression (L1)** reduces model complexity by shrinking some coefficients to zero, effectively performing feature selection. It's useful when many features are redundant or uninformative.\n",
    "- **Ridge Regression (L2)** keeps all features but distributes the weights more evenly, which helps when features are correlated.\n",
    "\n",
    "### 📈 Performance Summary\n",
    "\n",
    "| Model              | R² Score | Mean Squared Error (MSE) |\n",
    "|--------------------|----------|---------------------------|\n",
    "| Linear Regression  | ~0.925   | ~5306.88                  |\n",
    "| Lasso (L1)         | ~0.926   | ~5279.26                  |\n",
    "| Ridge (L2)         | ~0.942   | ~4080.82                  |\n",
    "\n",
    "- **Ridge Regression achieved the best performance** on the test set, with the highest R² and lowest MSE.\n",
    "- Visual inspection of coefficient magnitudes confirmed that regularization stabilizes model interpretation by penalizing overly large weights.\n",
    "\n",
    "📌 **Takeaway:** Regularization not only improves model performance but also makes the model more interpretable and robust—especially when working with highly correlated features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
